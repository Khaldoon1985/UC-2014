\documentclass[11pt]{extarticle}
\usepackage{slashbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage[makeroom]{cancel}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\newenvironment{Section}[2]{
  \section*{\huge{Section #1:\\ #2}}
}

\newcommand{\itab}[1]{\hspace{0em}\rlap{#1}}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf 20CS6037 Machine Learning \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1 (#2)  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}

%Send your finished notes to the instructor
%({\tt ancaralescu@gmail.com}), including the Latex file and all .eps
%or .pdf files that you've created for figures. Double-check that your file compiles with {\tt pdflatex lec1.tex} before you send it.

%These notes should be complete, correct, clear, and free of typos.

\begin{document}

\lecture{MLE, MAP, Bayesian Reasoning - Chapter 3\&5} {Lecture 6: 9/11/14}{Anca Ralescu}{Khaldoon Ashouiliy, Kyungmook Park}

\begin{Section}{1}{Bayesian Concept Learning}
\end{Section}
\begin{Section}{2}{The Beta Binomial Model}
\end{Section}
\begin{Section}{3}{Most Probable Classification}
\end{Section}
\begin{Section}{4}{The Gamma Distribution}
\end{Section}



\newpage

\begin{Section}{1}{Bayesian Concept Learning}
\Large{
D: Data ( set of example for a concept C)\\
ɦ: a point Hypothesis about C.\\
Note: That both p(D$|$h) D and ɦ can be viewed as functions from the set of instances to \{0,1\}\\
C: y  $\rightarrow$ \{0,1\}

\[
c(instance) =
\left\{ {\begin{array}{cc}
1 & if\;example\;of\;the\;concept\;C\\
0 & otherwise \\
\end{array} } \right.
\]

h and D are consistent if C(i) = h(i) $\forall$ i $\in$ Y\\

Bayes Theorem\\ 
			P(h$|$D) = $\frac{P(h|D)P(h)}{P(D)}$\\
			
How to choose hypotheses?\\

\underline{Correct the hypotheses?}
\begin{itemize}
\item
Correct on the training net.
\item
But not overfitting.
\end{itemize}

\underline{Example} Learning a real value function.\\
 
f: real valued function.\\
\itab{\underline{Training set}}\tab{D = \{($x_i, d_i$)$|d_i$=f($x_i$) + $e_i$\}}\\
\itab{}\tab{i=1,\ldots,m}\\

$e_i$ $\sim$ N (0, $\sigma_i$)\\

$\Rightarrow$ $h_{ML}$ = $\underset{h}{argmin}\sum_{i=1}^m[d_i-h(x_i)]^2$\\ 
\underline{Proof}\\
\itab{$h_{ML}$}\tab{=$\underset{h\in{H}}{argmax}P(D|h)$}\\
\itab{}\tab{=$\underset{h\in{H}}{argmax} P(d_i,\ldots,d_m|h)$ \underline{ind}}\\
\itab{}\tab{=$\underset{h\in{H}}{argmax} P(d_1|h) x\ldots{x}P(d_m|h)$}\\
\itab{}\tab{=$\underset{h\in{H}}{argmax} \prod_{i=1}^mP(d_i|h)$}\\
\itab{}\tab{=$\underset{h\in{H}}{argmax} \log[\prod_{i=1}^mP(d_i|h)]$}\\
\itab{}\tab{=$\underset{h\in{H}}{argmax} \sum_{i=1}^m\log{P(d_i|h)}$}\\

If iid N(0,$\sigma^2$)=0\\
Then, $d_i$ iid N(f($x_c$),$\sigma^2$)\\
iid = independent and identically distributed\\

from this point on we need to know the actual distribution of (di/h).\\

Use $e_i$ $\sim$ N (0, $\sigma_i$)\\

P($d_i|h$) = $\frac{1}{\sigma\sqrt{2\bar{u}}}e^{\frac{(d_i-h(x_i))^2}{2\sigma}}$\\

$\log{P(d_i|h)} = - \log{(\sigma\sqrt{2\bar{u}})}-\frac{1}{2\sigma}(d_i-h(x_i))^2$\\

\itab{$\Rightarrow\;h_{ML}$}\tab{=$\underset{h\in{H}}{argmax}\sum_{i=1}^m[-\log{(\sigma\sqrt{2\bar{u}})}-\frac{1}{2\sigma}(d_i-h(x_i))^2]$}\\
\itab{}\tab{=$\underset{h\in{H}}{argmax}[-\frac{1}{2\sigma}\sum_{i=1}^m(d_i-h(x_i))^2]$}\\
\itab{}\tab{=$\underset{h\in{H}}{argmax}\sum_{i=1}^m(d_i-h(x_i))^2$}\\
$(d_i-h(x_i))^2$ is the square error between f($x_i$) and h($x_i$)\\

\framebox[1.1\width]{$h_{ML} \equiv h_{square\;error}$} $e_i$ $\sim$ N (0, $\sigma_i$)

} %End of \Large
\end{Section}

\newpage

\begin{Section}{2}{The Beta Binomial Model}

\Large{

Learning to predict probabilities we want to learn f:X$\rightarrow$\{0,1\}\\
Define $P_0(x)$ = P(f(x) = 0 ; $P_1(x)$ = P(f(x) = 1) = ($-P_0(x)$)\\

\underline{example}\\
X = \{x$|$x is a patient with symptom\}\\

\[
f(x) =
\left\{ {\begin{array}{cc}
1 & if x serving \\
0 & otherwise \\
\end{array} } \right.
\]

We really want to learn the 'Concept' $P_1(x)$ = P(f(x)=1)\\
based on the learning data\\

D = \{$<x_i,d_i>,d_i=0\;or\;1\;i=1,\ldots,m$\}\\

What is P(D$|$h)?\\

\underline{Assume}\\
$x_i,d_i$ are random variables.\\
$x_i$ and h are independent.\\

\underline{Claim} The general\\
P($x_i,d_i|h$)=P($d_i|h,u_i$)P($x_i|h$)\\

\underline{Proof}\\
\itab{Right Hand Side}\tab{ = $P(d_i|h_i{u_i})P(x_i|h)$ = $\frac{P(d_i,h,u_i}{\cancel{P(h,u_i)}}\frac{\cancel{P(u_i,h)}}{P(h)}$}\\
\itab{}\tab{ = $P(d_i,x_i|h)$ = Left Hand Side}\\

Because $x_i$ and h are independent $\Rightarrow$\\
P(D$|$h) = $\prod_{i=1}^mP(x_i,d_i|h)=\prod_{i=1}^mP(d_i|h,u_i)P(x_i)$\\
Now \underline{P}$(d_i=1|h,u_i)=h(x_i)$\\
\[
\Rightarrow P(d_i|h,u_i) =
\left\{ {\begin{array}{cc}
h(u_i) & if\;d_i=1 \\
1-h(x_i) & d_i=0 \\
\end{array} } \right.
\]

$\Rightarrow$ $P(d_i|h(x_i))=[h(x_i)]^{d_i}[1-h(x_i)]^{1-d_i}$\\

$\Rightarrow$ P(D$|$h) = $\prod_{i=1}^m[h(x_i)]^{d_i}[1-h(x_i)]^{1-d_i}P(x_i)$\\

\itab{$h_{ML}$}\tab{=$\underset{h}{argmax}\prod_{i=1}^m[h(x_i)]^{d_i}[1-h(x_i)]^{1-d_i}P(x_i)$}\\
\itab{}\tab{=$\underset{h}{argmax}\underset{Negative Cross Entropy}{\sum_{i=1}^m[d_ih(x_i)+(1-d_i)[1-h(x_i)]+\cancel{\log{P(x_i)}}]}$}\\

} %End of \Large

\end{Section}
\begin{Section}{3}{Most Probable Classification}
\Large{
Suppose $P(h_1|D)=0.4$ $P(h_2|D)=0.3$ $P(h_3|D)=0.3$\\
$h_1(x)=+$, $h_2(x)=-$, $h_3(x)=-$\\
$h_MAP=h_1$; The Most Probable Classification\\

\underline{Bayes Optimal Classifier}\\
=$\underset{v \in V}{argmax}\sum_{h \in H}P(v|h)P(h|D)$\\

\centering
$v\in\{+,-\}$\\
\begin{table}[htbp]
	\centering
	\begin{tabular}{|c|c|c|c|}\hline
	\backslashbox{h}{P}	&	P(D$|$h)	&	P(-$|$h)	&	P(+$|$h)\\\hline
	$h_1$	&	0.4					&	0					&	1			\\\hline
	$h_2$	&	0.3					&	1					&	0			\\\hline
	$h_3$	&	0.3					&	1					&	0			\\\hline
	\end{tabular}
	\label{tab:booktabs}
\end{table}
\pagebreak
\flushleft
$\sum_{i=1}^3P(+|h_i)P(h_i|D)=1\times0.4+0\times0.3+0\times0.3=0.4$\\
$\sum_{i=1}^3P(-|h_i)P(h_i|D)=0\times0.4+1\times0.3+1\times0.3=0.6$\\

$\Rightarrow$ $\underset{v \in \{-,+\}}{argmax}\sum_{h \in H}P(v|h)P(h|D) = -$\\


} %End of Large


\end{Section}
\begin{Section}{4}{The Gamma Distribution}
\Large{
$X\in\Re+$ Random Variable $\sim$ $G(d>0,\beta>0)$\\
d = Shape, $\beta$ = rate\\

$f_{Gamma}(x)=\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta{x}}$          pdf\\\\

Where $\Gamma(t)=\int^\infty{u^{t-1}e^{-u}du}
\left( {\begin{array}{c}
\Gamma(t+1)=t\Gamma(t)\\
\forall{t}>0\\
\end{array} } \right)$\\

\itab{X $\sim$ Gamma($\alpha$,$\beta$)}\tab{}\tab{$\Rightarrow$ E(X) = $\frac{\alpha}{\beta}$;Var(X)=$\frac{\alpha}{\beta^2}$}\\
\itab{}\tab{}\tab{\;\;\;\;\;Mode(X)=$\frac{\alpha-1}{\beta}$}\\
$f'(x)=\frac{\beta^\alpha}{\Gamma(\alpha)}[(\alpha-1)x^{\alpha-2}e^{-\beta{x}}x^{\alpha-1}e^{-\beta{x}}]=0$\\
$x^{\alpha-2}e^{-\beta{x}}[\alpha-1-\beta{x}]\Rightarrow$\framebox[1.1\width]{x=$\frac{\alpha-1}{\beta}$}\\
E(X)=$\frac{\beta^\alpha}{\Gamma(\alpha)}\int_0^\infty{x^\alpha{e^{-\beta{x}}}dx\cdots}=\frac{\alpha}{\beta}$\\\\
\underline{The Beta Distribution}\\
X $\sim$ Beta (x$|\alpha,\beta$) = $\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}$\\
$B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$\\
E(X) = $\frac{\alpha}{\alpha+\beta}$\\
M(X) = $\frac{\alpha-1}{\alpha+\beta-2}$\\
Var(X) = $\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$\\
} %End of Large

\end{Section}

\end{document}

