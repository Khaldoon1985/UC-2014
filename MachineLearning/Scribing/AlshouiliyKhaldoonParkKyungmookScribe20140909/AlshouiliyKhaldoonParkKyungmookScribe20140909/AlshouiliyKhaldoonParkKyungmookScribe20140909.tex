\documentclass[11pt]{extarticle}
\usepackage{amssymb}
\usepackage{epsfig}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\newenvironment{Section}[2]{
  \section*{\huge{Section #1:\\ #2}}
}

\newcommand{\itab}[1]{\hspace{0em}\rlap{#1}}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf 20CS6037 Machine Learning \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1 (#2)  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}

%Send your finished notes to the instructor
%({\tt ancaralescu@gmail.com}), including the Latex file and all .eps
%or .pdf files that you've created for figures. Double-check that your file compiles with {\tt pdflatex lec1.tex} before you send it.

%These notes should be complete, correct, clear, and free of typos.

\begin{document}

\lecture{MLE, MAP, Bayesian Reasoning - Chapter 3 \& 5} {Lecture 5: 9/9/14}{Anca Ralescu}{Khaldoon Ashouiliy, Kyungmook Park}

\begin{Section}{1}{Conditional Independence}
\end{Section}
\begin{Section}{2}{Transformation of Random Variables}
\end{Section}
\begin{Section}{3}{General Transformations}
\end{Section}
\begin{Section}{4}{Monte Carlo Approximation}
\end{Section}
\begin{Section}{5}{Entropy}
\end{Section}
\begin{Section}{6}{Mutual Information}
\end{Section}



\newpage

\begin{Section}{1}{Conditional Independence}
\Large{X,Y r.v. X $\perp Y$ : X and Y are independent\\\\
\underline{Def}\\
X $\perp$ Y $\Leftrightarrow$ P(X,Y) = P(X)P(Y)\\
This really means \{w $\in$ S $|$ X(w) = a\}, \{w $\in$ S $|$ Y(w) = b\}\\
are independent event for all a $\in$ Range(x); b $\in$ Range(Y)

\underline{Notation}\\
X $\perp$ Y $|$ 2 : X and Y are \textbf{\textit{Conditionally Independent} (CI)} given 2\\

P(X $\perp$ Y $|$ 2) $\Leftrightarrow$ P(X, Y $|$ 2) = P(X $|$ 2)P(Y $|$ 2)\\

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
			\includegraphics[width=2in]{Figure1.png}
  % \includegraphics[width=2in]{name.pdf} % uncomment this line and put the figure in the same folder as this document.
   \caption{X and Y are CI given 2}
   \label{fig:Figure1}
\end{figure}
\underline{P}(X, Y) : Joint Distribution of X and Y\\
\{w $\in$ S $|$ X(w) = a\}, \{w $\in$ S $|$ Y(w) = b\}\\
Can extend to \underline{P}(X$_1$, $\ldots$ , X$_D$) : CDF, PDF/PMF\\
\itab{COV(X, Y)} \tab{$\triangleq$ E[(X-E(X))(Y-E(Y))]}\\
\itab{}\tab{ = E[XY - XE(Y) - YE(X) + E(X)E(Y)]}\\
\itab{}\tab{ = E(XY) = E(X)E(Y)}\\

\newpage

For a vector X = (X$_1$, X$_2$, X$_3$)\\

\[
Cov [X] =
\left[ {\begin{array}{ccc}
Var(X_1) & Cov(X_1,X_2) & Cov(X_1,X_3) \\
Cov(X_2,X_1) & Var(X_2) & Cov(X_2,X_3) \\
Cov(X_3,X_1) & Cov(X_3,X_2) & Var(X_3) \\
\end{array} } \right]
\]

Cov $\in$ (0, $\infty$)\\

\underline{Correlation} $\rho$(X,Y) = $\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$ $\in$ [-1,1]\\

X, Y independent $\Rightarrow$ Cov(X,Y) = 0 $\Rightarrow$ $\rho$(X,Y) = 0 (Uncorrelated)\\

Independence $\rightleftharpoons$ Uncorrelated\\

ex)\\
X $\sim$ u(-1,1) ; Y = X$^2$ $\Rightarrow$ X, Y are dependent\\
$\rho$(X,Y) = 0\\

E(X) = $\frac{-1+1}{2}$ = 0  Var(X) = $\frac{(1-(-1)^2}{12}$ = $\frac{4}{12}$ = $\frac{1}{3}$\\\\\\
\itab{E(Y)}\tab{= $\int_{-1}^1x^2f(x)dx$}\\
\itab{}\tab{= $\int_{-1}^1x^2(\frac{1}{2})dx$=$\frac{1}{2}\frac{x^4}{4}\arrowvert_{-1}^1 = \frac{2}{6} = \frac{1}{3}$}\\

E(XY) = $\int_{-1}^1x^3\frac{1}{2}dx = \frac{1}{2}\frac{x^4}{4}\arrowvert_{-1}^1 = \frac{1}{2}\cdot0$\\

Cov(X,Y) = E(XY) - E(X)E(Y) = 0 - 0 $\cdot$ $\frac{1}{3}$ = 0 - 0 = 0\\

} %End of \Large
\end{Section}

\newpage

\begin{Section}{2}{Transformation of Random Variables}

\Large{

X $\sim$ P(X)   p:pdf\\
Y = f(X)    What is the distribution of Y?\\
$P_Y(Y \leq y) = P_Y(f(X) \leq y) = \underline{P}_X(X \leq f^{-1}(y)) = P(f^{-1}(y))$\\

ex) Y = aX+b $\Rightarrow$ f(x) = aX+b\\

\itab{f$^{-1}$(y) = $\frac{y-b}{a}$}\tab{X = -1 Y = b-a ; X = 1 Y = a+b}\\

P$_Y$(y) = P$_X$ ($\frac{y-b}{a}$)\\

ex) X $\sim$ u(-1,1)

\[
P_X(x) =
\left\{ {\begin{array}{cc}
0 & x \leq -1 \\
\frac{1}{1-(2-1)} & -1 < X < 1 \\
0 & x \geq 1 \\
\end{array} } \right.
\]

\[
P_X(\frac{y-b}{a}) =
\left\{ {\begin{array}{cc}
0 & \frac{y-b}{a} \leq -1 \\
\frac{1}{1-(2-1)} & -1 < \frac{y-b}{a} < 1 \\
0 & x \geq 1 \\
\end{array} } \right.
= \left\{ {\begin{array}{cc}
0 & \frac{y-b}{a} \leq -1 \\
\frac{1}{1-(2-1)} & -1 < \frac{y-b}{a} < 1 \\
0 & x \geq 1 \\
\end{array} } \right.
\]

} %End of \Large

\end{Section}
\begin{Section}{3}{General Transformations}
\end{Section}
\begin{Section}{4}{Monte Carlo Approximation}
\end{Section}
\begin{Section}{5}{Entropy}
\end{Section}
\begin{Section}{6}{Mutual Information}
\end{Section}

\end{document}

